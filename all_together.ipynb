{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    subgraph Inputs\n",
    "        I1((x1))\n",
    "        I2((x2))\n",
    "    end\n",
    "\n",
    "    subgraph Hidden_Layer\n",
    "        H1[H1]\n",
    "        H2[H2]\n",
    "    end\n",
    "\n",
    "    subgraph Outputs\n",
    "        O((y_hat))\n",
    "        Y((y))\n",
    "        L((Loss))\n",
    "    end\n",
    "\n",
    "    I1 --> H1 & H2\n",
    "    I2 --> H1 & H2\n",
    "    H1 & H2 --> O\n",
    "    O & Y --> L\n",
    "\n",
    "    subgraph Backpropagation\n",
    "        L --> BO((dO))\n",
    "        BO --> BA1((dA1)) & BA2((dA2))\n",
    "        BA1 --> BW1((dW1))\n",
    "        BA2 --> BW2((dW2))\n",
    "        BW1 & BW2 --> BI1((dx1)) & BI2((dx2))\n",
    "    end\n",
    "\n",
    "    BI1 -.-> I1\n",
    "    BI2 -.-> I2\n",
    "\n",
    "    E1[\"Inputs:\n",
    "    x1, x2: Input features\"]\n",
    "    E2[\"Hidden Layer:\n",
    "    H1, H2: Hidden neurons\n",
    "    Each includes:\n",
    "    - Weighted Sum\n",
    "    - Activation function\"]\n",
    "    E3[\"Outputs:\n",
    "    y_hat: Predicted output\n",
    "    y: True label\n",
    "    Loss: Measure of prediction error\"]\n",
    "    E4[\"Backpropagation:\n",
    "    dO: Gradient of loss w.r.t. output\n",
    "    dA1, dA2: Gradients w.r.t. activations\n",
    "    dW1, dW2: Gradients w.r.t. weighted sums\n",
    "    dx1, dx2: Gradients w.r.t. inputs\"]\n",
    "\n",
    "    E1 -.-> Inputs\n",
    "    E2 -.-> Hidden_Layer\n",
    "    E3 -.-> Outputs\n",
    "    E4 -.-> Backpropagation\n",
    "\n",
    "    classDef default fill:#f5f5f5,stroke:#333,stroke-width:1px;\n",
    "    classDef input fill:#333,stroke:#000,stroke-width:2px,color:#fff;\n",
    "    classDef hidden fill:#ffeeba,stroke:#d39e00,stroke-width:2px,color:#000;\n",
    "    classDef output fill:#d5f5e3,stroke:#1e8449,stroke-width:2px,color:#000;\n",
    "    classDef backprop fill:#f5b7b1,stroke:#922b21,stroke-width:2px,color:#000;\n",
    "    classDef explanation fill:#e8e8e8,stroke:#333,stroke-width:1px,color:#333;\n",
    "\n",
    "    class I1,I2 input;\n",
    "    class H1,H2 hidden;\n",
    "    class O,Y,L output;\n",
    "    class BO,BA1,BA2,BW1,BW2,BI1,BI2 backprop;\n",
    "    class E1,E2,E3,E4 explanation;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(layer_dims):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        layer_dims (_type_): Dimensions of the layers in the Neural Network\n",
    "\n",
    "    Returns:\n",
    "        dict: Randomly initialized weights and biases for the Neural Network\n",
    "    \"\"\"\n",
    "    np.random.seed(3)\n",
    "    params = {}\n",
    "    L = len(layer_dims)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        params['W'+str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*0.01\n",
    "        params['b'+str(l)] = np.zeros((layer_dims[l], 1)) # Bias initilized as Zero\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m     cache \u001b[38;5;241m=\u001b[39m Z  \u001b[38;5;66;03m# Cache the value of Z for backpropagation\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m A, cache\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, sigmoid_orig(\u001b[38;5;241m1\u001b[39m))\n",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m, in \u001b[0;36msigmoid\u001b[0;34m(Z)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msigmoid\u001b[39m(Z):\n\u001b[0;32m---> 10\u001b[0m     A \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mexp(np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, Z))) \u001b[38;5;66;03m# The dot product multiple the matrix with -1.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     cache \u001b[38;5;241m=\u001b[39m (Z)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m A, cache\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Z (linear hypothesis) - Z = W*X + b , where\n",
    "# * - dot product,\n",
    "# W - weight matrix, b- bias vector, X- Input \n",
    "# This is a linear function called Sigmoid ativation function.\n",
    "# It is used to introduce non-linearity in the model.\n",
    "# Z can be a scalar a vector or a matrix.\n",
    "\n",
    "# In this case Z = -1 dot Z + b\n",
    "def sigmoid(Z):\n",
    "    \"\"\" This function computes the sigmoid of Z that in the NN context is the activation function of a neuron.\n",
    "        It is used to introduce non-linearity in the model.\n",
    "\n",
    "    Args:\n",
    "        Z (np.array): This is the linear hypothesis of the model.\n",
    "\n",
    "    Returns:\n",
    "        np.array, list: The sigmoid of Z and the cache of Z\n",
    "    \"\"\"\n",
    "    A = 1/(1+np.exp(np.dot(-1, Z))) # The dot product multiple the matrix with -1.\n",
    "    cache = (Z)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, params):\n",
    "    \"\"\" This function computes the forward propagation of the Neural Network.\n",
    "        It computes the linear hypothesis and applies the sigmoid activation function to the linear hypothesis.\n",
    "    \n",
    "    Args:\n",
    "        X (np.array): The training data\n",
    "        params (dict): The parameters of the Neural Network\n",
    "    Returns:\n",
    "        np.array, list: The output of the Neural Network and the caches of the linear and activation functions \n",
    "    \"\"\"    \n",
    "\n",
    "    A = X # input to first layer i.e. training data\n",
    "    caches = []\n",
    "    L = len(params)//2 # The number of layers in the network. Each layer has a weight and a bias, so the total number of parameters is twice the number of layers.\n",
    "    print(f'Length of params: {len(params)}, L: {L}, Params: {params}')\n",
    "    for l in range(1, L+1):\n",
    "        A_prev = A\n",
    "\n",
    "        # Debug\n",
    "        print(f\"Shape of W{str(l)}): {params['W'+str(l)].shape}\")\n",
    "        print(f\"Shape of A_prev: {A_prev.shape}\")\n",
    "        \n",
    "        # Linear Hypothesis - Using the formula Z = W*X + b\n",
    "        print(f'Weight {\"W\"+str(l)}: {params[\"W\"+str(l)]}, A_prev: {A_prev}, Bias: {params[\"b\"+str(l)]}')\n",
    "        Z = np.dot(params['W'+str(l)], A_prev) + params['b'+str(l)] \n",
    "\n",
    "        # Storing the linear cache\n",
    "        linear_cache = (A_prev, params['W'+str(l)], params['b'+str(l)]) \n",
    "        print(f'Linear Cache: {linear_cache}')\n",
    "        # Applying sigmoid on linear hypothesis\n",
    "        A, activation_cache = sigmoid(Z) \n",
    "        print(f'Sigmoid: {A}, Activation Cache: {activation_cache}')\n",
    "         # storing the both linear and activation cache\n",
    "        cache = (linear_cache, activation_cache)\n",
    "        caches.append(cache)\n",
    "\n",
    "    return A, caches\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "understanding_neural_networks-sF0B2dIY",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
