{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks from scratch\n",
    "https://www.freecodecamp.org/news/building-a-neural-network-from-scratch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(layer_dims):\n",
    "    np.random.seed(3)\n",
    "    params = {}\n",
    "    L = len(layer_dims)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        params['W'+str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*0.01\n",
    "        params['b'+str(l)] = np.zeros((layer_dims[l], 1)) # Bias initilized as Zero\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.01788628]]), 'b1': array([[0.]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = init_params([1,1])\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ( W ) (Weight Matrix):\n",
    "\n",
    "This is a matrix of weights. Each element in this matrix represents the strength of the connection between the input features and the neurons in the layer.\n",
    "\n",
    "Dimensions: If you have ( n ) input features and ( m ) neurons in the layer, ( W ) will be an ( m * n )matrix.\n",
    "\n",
    "# ( X ) (Input Vector/Matrix):\n",
    "\n",
    "This is the input data. It can be a vector (for a single data point) or a matrix (for multiple data points).\n",
    "Dimensions: If you have ( n ) input features, ( X ) will be an ( n * 1 ) vector for a single data point or an ( n * k ) matrix for ( k ) data points.\n",
    "\n",
    "# ( b ) (Bias Vector):\n",
    "\n",
    "This is a vector of biases. Each element in this vector is added to the corresponding neuron's weighted sum to shift the activation function.\n",
    "Dimensions: If you have ( m ) neurons, ( b ) will be an ( m * 1 ) vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ( W ) (Weight Matrix):\n",
    "\n",
    "This is a matrix of weights. Each element in this matrix represents the strength of the connection between the input features and the neurons in the layer.\n",
    "\n",
    "Dimensions: If you have ( n ) input features and ( m ) neurons in the layer, ( W ) will be an ( m \\times n ) matrix.\n",
    "\n",
    "# ( X ) (Input Vector/Matrix):\n",
    "\n",
    "This is the input data. It can be a vector (for a single data point) or a matrix (for multiple data points).\n",
    "Dimensions: If you have ( n ) input features, ( X ) will be an ( n \\times 1 ) vector for a single data point or an ( n \\times k ) matrix for ( k ) data points.\n",
    "\n",
    "# ( b ) (Bias Vector):\n",
    "\n",
    "This is a vector of biases. Each element in this vector is added to the corresponding neuron's weighted sum to shift the activation function.\n",
    "Dimensions: If you have ( m ) neurons, ( b ) will be an ( m \\times 1 ) vector.\n",
    "\n",
    "### Example\n",
    "\n",
    "Consider a simple example with a single data point:\n",
    "\n",
    "- **Input Vector \\( X \\)**: \n",
    "  \\[\n",
    "  \\begin{bmatrix}\n",
    "  x_1 \\\\\n",
    "  x_2\n",
    "  \\end{bmatrix}\n",
    "  \\]\n",
    "\n",
    "- **Weight Matrix \\( W \\)**: \n",
    "  \\[\n",
    "  \\begin{bmatrix}\n",
    "  w_{11} & w_{12} \\\\\n",
    "  w_{21} & w_{22}\n",
    "  \\end{bmatrix}\n",
    "  \\]\n",
    "\n",
    "- **Bias Vector \\( b \\)**: \n",
    "  \\[\n",
    "  \\begin{bmatrix}\n",
    "  b_1 \\\\\n",
    "  b_2\n",
    "  \\end{bmatrix}\n",
    "  \\]\n",
    "\n",
    "The computation would be:\n",
    "\n",
    "\\[\n",
    "Z = W dot X + b = \n",
    "\\begin{bmatrix}\n",
    "w_{11} & w_{12} \\\\\n",
    "w_{21} & w_{22}\n",
    "\\end{bmatrix}\n",
    "dot\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "b_1 \\\\\n",
    "b_2\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "Breaking it down:\n",
    "\n",
    "\\[\n",
    "Z = \n",
    "\\begin{bmatrix}\n",
    "(w_{11} \\cdot x_1 + w_{12} \\cdot x_2) \\\\\n",
    "(w_{21} \\cdot x_1 + w_{22} \\cdot x_2)\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "b_1 \\\\\n",
    "b_2\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "Linear Transformation\n",
    "The equation Z = W * X + b represents a linear transformation of the input X using the weight matrix W and the bias vector b. This is a common step in neural networks before applying an activation function.\n",
    "\n",
    "\\[\n",
    "Z = \n",
    "\\begin{bmatrix}\n",
    "(w_{11} \\cdot x_1 + w_{12} \\cdot x_2 + b_1) \\\\\n",
    "(w_{21} \\cdot x_1 + w_{22} \\cdot x_2 + b_2)\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "Sigmoid Function\n",
    "The expression 1/(1+np.exp(np.dot(-1, Z))) is the sigmoid activation function applied to Z. The sigmoid function is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma(Z) = \\frac{1}{1 + e^{-Z}} \n",
    "\\end{equation}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7310585786300049, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Z (linear hypothesis) - Z = W*X + b , \n",
    "# * - dot product,\n",
    "# W - weight matrix, b- bias vector, X- Input \n",
    "# This is a linear function called Sigmoid ativation function.\n",
    "# It is used to introduce non-linearity in the model.\n",
    "# Z can be a scalar a vector or a matrix.\n",
    "\n",
    "# In this case Z = -1 dot Z + b\n",
    "def sigmoid(Z):\n",
    "    A = 1/(1+np.exp(np.dot(-1, Z))) # The dot product multiple the matrix with -1.\n",
    "    cache = (Z)\n",
    "\n",
    "    return A, cache\n",
    "  \n",
    "sigmoid(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -2, -3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = np.array([1, 2, 3])\n",
    "np.dot(-1, Z) # This is equal to -1*Z or -Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.36787944, 0.13533528, 0.04978707])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(np.dot(-1, Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z: [[1.9]\n",
      " [0.9]]\n",
      "A: [[0.86989153]\n",
      " [0.7109495 ]]\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "import numpy as np\n",
    "\n",
    "# Define the weight matrix, input vector, and bias vector\n",
    "W = np.array([[0.2, 0.8], [0.5, 0.1]])\n",
    "X = np.array([[1], [2]])\n",
    "b = np.array([[0.1], [0.2]])\n",
    "\n",
    "# Perform the linear transformation\n",
    "Z = np.dot(W, X) + b\n",
    "\n",
    "# Apply the sigmoid activation function\n",
    "A = 1 / (1 + np.exp(-Z)) # - is used to multiply the matrix with -1 and works seemlessly also with arrays\n",
    "\n",
    "print(\"Z:\", Z)\n",
    "print(\"A:\", A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, params):\n",
    "\n",
    "    A = X # input to first layer i.e. training data\n",
    "    caches = []\n",
    "    L = len(params)//2 # The number of layers in the network. Each layer has a weight and a bias, so the total number of parameters is twice the number of layers.\n",
    "    print(f'Length of params: {len(params)}, L: {L}, Params: {params}')\n",
    "    for l in range(1, L+1):\n",
    "        A_prev = A\n",
    "\n",
    "        # Linear Hypothesis - Using the formula Z = W*X + b\n",
    "        print(f'Weight {\"W\"+str(l)}: {params[\"W\"+str(l)]}, A_prev: {A_prev}, Bias: {params[\"b\"+str(l)]}')\n",
    "        Z = np.dot(params['W'+str(l)], A_prev) + params['b'+str(l)] \n",
    "\n",
    "        # Storing the linear cache\n",
    "        linear_cache = (A_prev, params['W'+str(l)], params['b'+str(l)]) \n",
    "        print(f'Linear Cache: {linear_cache}')\n",
    "        # Applying sigmoid on linear hypothesis\n",
    "        A, activation_cache = sigmoid(Z) \n",
    "        print(f'Sigmoid: {A}, Activation Cache: {activation_cache}')\n",
    "         # storing the both linear and activation cache\n",
    "        cache = (linear_cache, activation_cache)\n",
    "        caches.append(cache)\n",
    "\n",
    "    return A, caches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of params: 4, L: 2, Params: {'W1': array([[0.2, 0.8],\n",
      "       [0.5, 0.1]]), 'b1': array([[0.1],\n",
      "       [0.2]]), 'W2': array([[0.3, 0.7]]), 'b2': array([[0.3]])}\n",
      "Weight W1: [[0.2 0.8]\n",
      " [0.5 0.1]], A_prev: [[1 2]\n",
      " [3 4]], Bias: [[0.1]\n",
      " [0.2]]\n",
      "Linear Cache: (array([[1, 2],\n",
      "       [3, 4]]), array([[0.2, 0.8],\n",
      "       [0.5, 0.1]]), array([[0.1],\n",
      "       [0.2]]))\n",
      "Sigmoid: [[0.93702664 0.97587298]\n",
      " [0.73105858 0.83201839]], Activation Cache: [[2.7 3.7]\n",
      " [1.  1.6]]\n",
      "Weight W2: [[0.3 0.7]], A_prev: [[0.93702664 0.97587298]\n",
      " [0.73105858 0.83201839]], Bias: [[0.3]]\n",
      "Linear Cache: (array([[0.93702664, 0.97587298],\n",
      "       [0.73105858, 0.83201839]]), array([[0.3, 0.7]]), array([[0.3]]))\n",
      "Sigmoid: [[0.74891783 0.7640791 ]], Activation Cache: [[1.092849   1.17517476]]\n",
      "\n",
      "A:\t[[0.74891783 0.7640791 ]]\n",
      "Caches:\t[((array([[1, 2],\n",
      "       [3, 4]]), array([[0.2, 0.8],\n",
      "       [0.5, 0.1]]), array([[0.1],\n",
      "       [0.2]])), array([[2.7, 3.7],\n",
      "       [1. , 1.6]])), ((array([[0.93702664, 0.97587298],\n",
      "       [0.73105858, 0.83201839]]), array([[0.3, 0.7]]), array([[0.3]])), array([[1.092849  , 1.17517476]]))]\n"
     ]
    }
   ],
   "source": [
    "A, caches = forward_prop(np.array([[1,2],[3,4]]), params)\n",
    "\n",
    "print(f\"\\nA:\\t{A}\\nCaches:\\t{caches}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "`X=[[1,2],[3,4]]`\n",
    "\n",
    "```yaml\n",
    "    params={\n",
    "        'W1': array([[0.2, 0.8], [0.5, 0.1]]), \n",
    "        'b1': array([[0.1], [0.2]]),\n",
    "        'W2': array([[0.3, 0.7]]),\n",
    "        'b2': array([[0.3]])\n",
    "    }\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    subgraph Input Layer\n",
    "        X1[1] -->|W1| Z1\n",
    "        X2[2] -->|W1| Z1\n",
    "        X3[3] -->|W1| Z2\n",
    "        X4[4] -->|W1| Z2\n",
    "    end\n",
    "\n",
    "    subgraph Hidden Layer 1\n",
    "        Z1[Z1 = 0.2*1 + 0.8*2] -->| b1| Z1b\n",
    "        Z2[Z2 = 0.5*3 + 0.1*4] -->| b1| Z2b\n",
    "        Z1b[Z1b = Z1 + 0.1] -->|Sigmoid| A1\n",
    "        Z2b[Z2b = Z2 + 0.2] -->|Sigmoid| A2\n",
    "    end\n",
    "\n",
    "    subgraph Output Layer\n",
    "        A1 -->|W2| Z3\n",
    "        A2 -->|W2| Z3\n",
    "        Z3[Z3 = 0.3*A1 + 0.7*A2] -->| b2| Z3b\n",
    "        Z3b[Z3b = Z3 + 0.3] -->|Sigmoid| A3\n",
    "    end\n",
    "\n",
    "    A3 --> FinalOutput[Output A3]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "```mermaid\n",
    "%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#ffcc00', 'edgeLabelBackground':'#ffffff', 'tertiaryColor': '#ffffff'}}}%%\n",
    "graph LR\n",
    "    subgraph Column1\n",
    "        A[Input Vector/Matrix #40;X#41;] --> B[Weight Matrix #40;W#41;]\n",
    "        A --> C[Bias Vector #40;b#41;]\n",
    "        B --> D[Linear Transformation #40;Z = W * X + b#41;]\n",
    "        C --> D\n",
    "        D --> E[Activation Function #40;A = sigmoid#40;Z#41;#41;]\n",
    "        E --> F[Output #40;A#41;]\n",
    "    end\n",
    "\n",
    "    subgraph Column2\n",
    "        B_explanation[Weights applied to the input data]\n",
    "        C_explanation[Biases added to the weighted sum]\n",
    "        D_explanation[Linear combination of weights, inputs, and biases]\n",
    "        E_explanation[Activation function applied to the linear transformation]\n",
    "        F_explanation[Final output after applying the activation function]\n",
    "\n",
    "        B -.-> B_explanation\n",
    "        C -.-> C_explanation\n",
    "        D -.-> D_explanation\n",
    "        E -.-> E_explanation\n",
    "        F -.-> F_explanation\n",
    "    end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of params: 4, L: 2, Params: {'W1': array([[0.2, 0.8],\n",
      "       [0.5, 0.1]]), 'b1': array([[0.1],\n",
      "       [0.2]]), 'W2': array([[0.3, 0.7]]), 'b2': array([[0.3]])}\n",
      "Weight W1: [[0.2 0.8]\n",
      " [0.5 0.1]], A_prev: [[1 2]\n",
      " [3 4]], Bias: [[0.1]\n",
      " [0.2]]\n",
      "Linear Cache: (array([[1, 2],\n",
      "       [3, 4]]), array([[0.2, 0.8],\n",
      "       [0.5, 0.1]]), array([[0.1],\n",
      "       [0.2]]))\n",
      "Sigmoid: [[0.93702664 0.97587298]\n",
      " [0.73105858 0.83201839]], Activation Cache: [[2.7 3.7]\n",
      " [1.  1.6]]\n",
      "Weight W2: [[0.3 0.7]], A_prev: [[0.93702664 0.97587298]\n",
      " [0.73105858 0.83201839]], Bias: [[0.3]]\n",
      "Linear Cache: (array([[0.93702664, 0.97587298],\n",
      "       [0.73105858, 0.83201839]]), array([[0.3, 0.7]]), array([[0.3]]))\n",
      "Sigmoid: [[0.74891783 0.7640791 ]], Activation Cache: [[1.092849   1.17517476]]\n",
      "Final Activation: [[0.74891783 0.7640791 ]]\n",
      "Caches: [((array([[1, 2],\n",
      "       [3, 4]]), array([[0.2, 0.8],\n",
      "       [0.5, 0.1]]), array([[0.1],\n",
      "       [0.2]])), array([[2.7, 3.7],\n",
      "       [1. , 1.6]])), ((array([[0.93702664, 0.97587298],\n",
      "       [0.73105858, 0.83201839]]), array([[0.3, 0.7]]), array([[0.3]])), array([[1.092849  , 1.17517476]]))]\n"
     ]
    }
   ],
   "source": [
    "# Example with 2 layers\n",
    "import numpy as np\n",
    "\n",
    "# Example parameters\n",
    "params = {\n",
    "    'W1': np.array([[0.2, 0.8], [0.5, 0.1]]),\n",
    "    'b1': np.array([[0.1], [0.2]]),\n",
    "    'W2': np.array([[0.3, 0.7]]),\n",
    "    'b2': np.array([[0.3]])\n",
    "}\n",
    "\n",
    "# Example input\n",
    "X = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "# Perform forward propagation\n",
    "A, caches = forward_prop(X, params)\n",
    "\n",
    "print(\"Final Activation:\", A)\n",
    "print(\"Caches:\", caches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's now define our cost function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
